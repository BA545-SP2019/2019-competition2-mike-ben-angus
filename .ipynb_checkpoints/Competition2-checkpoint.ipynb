{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data(object):\n",
    "    def __init__(self,file,csv_indicator):\n",
    "        self.file = file\n",
    "        self.csv_indicator = csv_indicator\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        if self.csv_indicator==1:\n",
    "            self.df = pd.read_csv(self.file,skiprows=1).drop('ID',1)\n",
    "        else:\n",
    "            self.df = pd.read_excel(self.file,skiprows=1).drop('ID',1)\n",
    "        \n",
    "    #column X3 or education should only have values 1-4. \n",
    "    #take out the 5,6,7 values that show up\n",
    "    #what should our methodology be for replacing these values?\n",
    "    #random selection?  Ask Tao what we should do with these values.\n",
    "    #i think randomly selecting a value (1-4) based on actual distribution of column makes sense\n",
    "    #to do this calculate length of distribution w/ acceptable values\n",
    "    #generate random number inside length of that distribution\n",
    "    #index acceptable values by random number you generated\n",
    "    def _fix_education_values(self):\n",
    "        def replace_values(x):\n",
    "            if x > 4:\n",
    "                return 4\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        self.df['EDUCATION'] = [replace_values(x) for x in self.df.EDUCATION]\n",
    "        \n",
    "    \n",
    "    #should have 1 (married),2 (single),3 (others)\n",
    "    #replace all 0's with 3 as they will fall under the category \"others\"\n",
    "    def _fix_marital_status(self):\n",
    "        def fix_numbers(x):\n",
    "            if x == 0:\n",
    "                return 3\n",
    "            else:\n",
    "                return x\n",
    "        self.df['MARRIAGE'] = [fix_numbers(x) for x in self.df.MARRIAGE]\n",
    "       \n",
    "        \n",
    "    def _fix_late_pay_status(self,include_columns):\n",
    "        for column in include_columns:\n",
    "            def max_(x):\n",
    "                return max(x,0)\n",
    "            self.df[column] = [max_(x) for x in self.df[column]]\n",
    "        #columns 6-11\n",
    "        #anything negative should be switched to 0. This indicates they have paid on time and in some cases two months early.\n",
    "        #columns 12-17\n",
    "        #this could also be used on these columns. Anything negative indicates they have overpayed. Turn the columns into...\n",
    "        #...money due so replace all negative values.\n",
    "        \n",
    "        \n",
    "    def _fix_outliers(self,exclude_columns=None):\n",
    "        def get_outliers(self,value,lower,upper):\n",
    "            if value >= upper + 1.5*upper:\n",
    "                return upper\n",
    "            elif value <= lower - 1.5*lower:\n",
    "                return lower\n",
    "            else:\n",
    "                return value\n",
    "    \n",
    "        if exclude_columns==None:\n",
    "            unique_cols = self.df.columns\n",
    "        else:\n",
    "            unique_cols = self.df.columns.drop(exclude_columns,1)\n",
    "        for i in unique_cols:\n",
    "            lower = self.df[i].quantile(0.225)\n",
    "            upper = self.df[i].quantile(0.775)\n",
    "            self.df[i]= [get_outliers(self,value,lower,upper) \n",
    "                         for value,lower,upper in \n",
    "                         zip(self.df[i],\n",
    "                             [lower for x in range(0,len(self.df))],\n",
    "                             [upper for y in range(0,len(self.df))]\n",
    "                            )\n",
    "                        ]\n",
    "            \n",
    "    \n",
    "    #def _normalize_stuff(self,exclude_columns=None):\n",
    "        #how do we make up a set of rules that checks for skewness and applies a certain function to the distribution\n",
    "        #if that skewness still isn't within acceptable bounds try different function\n",
    "    \n",
    "    #scale all the columns (0-1)\n",
    "    def _scale_stuff(self,exclude_columns=None):\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        if exclude_columns==None:\n",
    "            unique_cols = self.df.columns\n",
    "        else:\n",
    "            unique_cols = self.df.columns.drop(exclude_columns,1)\n",
    "        \n",
    "        for col in unique_cols:\n",
    "            self.df[col] = pd.Series(min_max_scaler.fit_transform(np.array(self.df[col]).reshape(-1,1)).reshape(1,len(self.df[col]))[0])\n",
    "            \n",
    "    def _calculate_z_scores(self,exclude_columns=None):\n",
    "        if exclude_columns== None:\n",
    "            unique_cols = self.df.columns\n",
    "        else:\n",
    "            unique_cols = self.df.columns.drop(exclude_columns,1)\n",
    "        for col in unique_cols:\n",
    "            self.df[col] = (self.df[col] - self.df[col].mean())/self.df[col].std(ddof=0)\n",
    "    \n",
    "    def _one_hot_encode(self,include_columns=None):\n",
    "        if include_columns == None:\n",
    "            print('pass the list of columns you want to one hot encode')\n",
    "        else:\n",
    "            self.df = pd.get_dummies(self.df, prefix_sep=\"__\",\n",
    "                              columns=include_columns)\n",
    "        \n",
    "            \n",
    "        #when new data production data is introduced drop anything not in column_list\n",
    "        column_list = self.df.columns\n",
    "        \n",
    "    #def _calculate_x_features():\n",
    "        \n",
    "    #def _calculate_target_variables():\n",
    "        \n",
    "    #def _bin_column_values(column_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data_Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMAL WORK FLOW EXAMPLE BELOW\n",
    "##PIPELINE 1\n",
    "\n",
    "#include_columns = [Columns X6-X11]  \n",
    "include_columns = ['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "#include_columns1 = [Columns X12-X17]\n",
    "include_columns1 = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n",
    "#categorical = [Categorical Columns]\n",
    "categorical = ['SEX','EDUCATION','MARRIAGE','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','default payment next month']\n",
    "\n",
    "#initialize object\n",
    "F = data('default_of_credit_card_clients.xlsx',0)\n",
    "\n",
    "#fix known bad stuff, maybe call these in the initialize function so you don't have to explicitly call them\n",
    "F._fix_marital_status()\n",
    "F._fix_late_pay_status(include_columns)\n",
    "F._fix_late_pay_status(include_columns1)\n",
    "F._fix_education_values() \n",
    "       \n",
    "    \n",
    "#THEN IN ANY ORDER, TRY DIFFERENT ORDERS OUT TO SEE WHAT WORKS BEST\n",
    "F._fix_outliers(categorical)\n",
    "#F._normalize_stuff(categorical)\n",
    "F._scale_stuff(categorical)\n",
    "F._calculate_z_scores(categorical)\n",
    "F._one_hot_encode(['SEX','EDUCATION','MARRIAGE'])\n",
    "\n",
    "#use above datdaframe to then test which order produces best modeling performance\n",
    "#take modeling code from evaluation code in competition1 (xgboost model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'scal_pos_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8af3867d34eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                       \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary:logistic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                       \u001b[0mscal_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                       seed=27)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#Oversample data since it's imbalanced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'scal_pos_weight'"
     ]
    }
   ],
   "source": [
    "y = F.df['default payment next month']\n",
    "X = F.df.drop('default payment next month',1)\n",
    "#y = H.df['default payment next month']\n",
    "#X = H.df.drop('default payment next month',1)\n",
    "\n",
    "from tqdm import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt \n",
    "from xgboost import plot_importance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "scores = []\n",
    "f1 = []\n",
    "count=0\n",
    "for i in tqdm(range(0,10)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    model = XGBClassifier(learning_rate = 0.1,\n",
    "                      n_estimators=100,\n",
    "                      max_depth=3,\n",
    "                      min_child_weight=3,\n",
    "                      gamma=1.0,\n",
    "                      reg_alpha=0.05,\n",
    "                      colsample_bytree=1.0,\n",
    "                      objective='binary:logistic',\n",
    "                      scal_pos_weight=1,\n",
    "                      seed=27)\n",
    "\n",
    "    #Oversample data since it's imbalanced\n",
    "    sm=SMOTE(random_state=i,ratio=1.0)\n",
    "    x_train_res, y_train_res=sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "    model.fit(x_train_res,y_train_res)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores.append(accuracy)\n",
    "        \n",
    "    f1_score_ = f1_score(y_test,y_pred)\n",
    "    f1.append(f1_score_)\n",
    "\n",
    "print(scores)\n",
    "print('')\n",
    "print('Accuracy' + ' ' + str(np.mean(scores)))\n",
    "print('')\n",
    "print(f1)\n",
    "print('')\n",
    "print('F1' + ' ' + str(np.mean(f1)))\n",
    "#H has avg accuracy of .8229\n",
    "\n",
    "\n",
    "#only 20% of people defaulted.\n",
    "#If we guessed \"not default\" everytime we'd be about 80% correct so the above results actually kind of suck\n",
    "1-sum(F.df['default payment next month'])/len(F.df['default payment next month'])\n",
    "plot_importance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###PIPELINE 2\n",
    "##PIPELINE 1\n",
    "\n",
    "#include_columns = [Columns X6-X11]  \n",
    "include_columns = ['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "#include_columns1 = [Columns X12-X17]\n",
    "include_columns1 = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n",
    "#categorical = [Categorical Columns]\n",
    "categorical = ['SEX','EDUCATION','MARRIAGE','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','default payment next month']\n",
    "\n",
    "#initialize object\n",
    "H = data('default_of_credit_card_clients.xlsx',0)\n",
    "\n",
    "#fix known bad stuff, maybe call these in the initialize function\n",
    "H._fix_marital_status()\n",
    "H._fix_late_pay_status(include_columns)\n",
    "H._fix_late_pay_status(include_columns1)\n",
    "H._fix_education_values() \n",
    "       \n",
    "    \n",
    "#THEN IN ANY ORDER, TRY DIFFERENT ORDERS OUT TO SEE WHAT WORKS BEST\n",
    "#H._fix_outliers(categorical)\n",
    "#F._normalize_stuff(categorical)\n",
    "#H._scale_stuff(categorical)\n",
    "#H._calculate_z_scores(categorical)\n",
    "H._one_hot_encode(['SEX','EDUCATION','MARRIAGE'])\n",
    "\n",
    "#use above datdaframe to then test which order produces best modeling performance\n",
    "#take modeling code from evaluation code in competition1 (xgboost model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'scal_pos_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e9e526e587d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                       \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary:logistic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                       \u001b[0mscal_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                       seed=27)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#Oversample data since it's imbalanced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'scal_pos_weight'"
     ]
    }
   ],
   "source": [
    "#y = F.df['default payment next month']\n",
    "#X = F.df.drop('default payment next month',1)\n",
    "y1 = H.df['default payment next month']\n",
    "X1 = H.df.drop('default payment next month',1)\n",
    "\n",
    "from tqdm import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt \n",
    "from xgboost import plot_importance\n",
    "\n",
    "scores1 = []\n",
    "f1_1 = []\n",
    "count=0\n",
    "for i in tqdm(range(0,10)):\n",
    "    X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=i)\n",
    "    model1 = XGBClassifier(learning_rate = 0.1,\n",
    "                      n_estimators=100,\n",
    "                      max_depth=3,\n",
    "                      min_child_weight=3,\n",
    "                      gamma=1.0,\n",
    "                      reg_alpha=0.05,\n",
    "                      colsample_bytree=1.0,\n",
    "                      objective='binary:logistic',\n",
    "                      scal_pos_weight=1,\n",
    "                      seed=27)\n",
    "    \n",
    "    #Oversample data since it's imbalanced\n",
    "    sm=SMOTE(random_state=i,ratio=1.0)\n",
    "    x_train_res1, y_train_res1=sm.fit_sample(X_train1, y_train1)\n",
    "\n",
    "    model1.fit(x_train_res1,y_train_res1)\n",
    "    y_pred1 = model1.predict(X_test1)\n",
    "\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores1.append(accuracy)\n",
    "        \n",
    "    f1_score_ = f1_score(y_test,y_pred)\n",
    "    f1_1.append(f1_score_)\n",
    "\n",
    "print(scores1)\n",
    "print('')\n",
    "print('Accuracy' + ' ' + str(np.mean(scores1)))\n",
    "print(' ')\n",
    "print(f1_1)\n",
    "print('')\n",
    "print('F1' + ' ' + str(np.mean(f1_1)))\n",
    "#H has avg accuracy of .8229\n",
    "\n",
    "#only 20% of people defaulted.\n",
    "#If we guessed \"not default\" everytime we'd be about 80% correct so the above results actually kind of suck\n",
    "1-sum(F.df['default payment next month'])/len(F.df['default payment next month'])\n",
    "plot_importance(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamingonzalez/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7753333333333333\n",
      "\n",
      "0.5209665955934613\n",
      "\n",
      "PAY_0. feature 2 (0.2919192697230052)\n",
      "PAY_2. feature 3 (0.16699337992735383)\n",
      "PAY_5. feature 6 (0.10630818449971637)\n",
      "PAY_3. feature 4 (0.08560089777795493)\n",
      "PAY_4. feature 5 (0.08414039781451303)\n",
      "PAY_6. feature 7 (0.07138845229773598)\n",
      "PAY_AMT1. feature 14 (0.040521770788816254)\n",
      "LIMIT_BAL. feature 0 (0.03531997624567943)\n",
      "PAY_AMT2. feature 15 (0.024472316927913882)\n",
      "PAY_AMT3. feature 16 (0.023791196200580043)\n",
      "PAY_AMT6. feature 19 (0.012833406121569392)\n",
      "PAY_AMT4. feature 17 (0.010865289630515156)\n",
      "PAY_AMT5. feature 18 (0.009239866164909192)\n",
      "MARRIAGE__1. feature 27 (0.008145519491378287)\n",
      "EDUCATION__2. feature 24 (0.006881746137276515)\n",
      "SEX__1. feature 20 (0.0047152936481437095)\n",
      "EDUCATION__1. feature 23 (0.004155196689562885)\n",
      "BILL_AMT4. feature 11 (0.0039040122291934716)\n",
      "BILL_AMT2. feature 9 (0.002523793642999646)\n",
      "BILL_AMT3. feature 10 (0.0013328201026243788)\n",
      "MARRIAGE__2. feature 28 (0.0013256428278039317)\n",
      "BILL_AMT1. feature 8 (0.0010379738621710268)\n",
      "BILL_AMT6. feature 13 (0.0010165623866825767)\n",
      "EDUCATION__4. feature 26 (0.0008339626253909345)\n",
      "SEX__2. feature 21 (0.0004129649449270889)\n",
      "AGE. feature 1 (0.00032010729158282497)\n",
      "EDUCATION__3. feature 25 (0.0)\n",
      "EDUCATION__0. feature 22 (0.0)\n",
      "BILL_AMT5. feature 12 (0.0)\n",
      "MARRIAGE__3. feature 29 (0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100,max_depth=2,random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "\n",
    "#Oversample data since it's imbalanced\n",
    "sm=SMOTE(random_state=i,ratio=1.0)\n",
    "x_train_res, y_train_res=sm.fit_sample(X_train, y_train)\n",
    "\n",
    "clf.fit(x_train_res,y_train_res)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy2 = accuracy_score(y_test, y_pred)\n",
    "f1_score2 = f1_score(y_test,y_pred)\n",
    "print(accuracy2)\n",
    "print('')\n",
    "print(f1_score2)\n",
    "print('')\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"{}. feature {} ({})\".format(X_train.columns[indices[f]],indices[f],importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab4e9e76278e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2 = RandomForestClassifier(n_estimators=100,max_depth=3,random_state=0)\n",
    "clf2.fit(X_train,y_train)\n",
    "y_pred = clf2.predict(X_test)\n",
    "accuracy2 = accuracy_score(y_test, y_pred)\n",
    "f1_score2 = f1_score(y_test,y_pred)\n",
    "print(accuracy2)\n",
    "print('')\n",
    "print(f1_score2)\n",
    "print('')\n",
    "\n",
    "importances = clf2.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"{}. feature {} ({})\".format(X_train.columns[indices[f]],indices[f],importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamingonzalez/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6e0af243d5b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m               }\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#param_test1 = {'max_depth':[10],'min_child_weight':[1],'reg_alpha':[0.05],'gamma':[0.5],'colsample_bytree':[1]}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgsearch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_test1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "param_test1 = {\n",
    "               'max_depth':[3,7,10],\n",
    "               'min_child_weight':[1,3,6],\n",
    "               'gamma':[0,0.5,1.0],\n",
    "               'colsample_bytree':[0,0.5,1.0],\n",
    "               'reg_alpha':[0,0.005,0.01,0.05]\n",
    "              }\n",
    "#param_test1 = {'max_depth':[10],'min_child_weight':[1],'reg_alpha':[0.05],'gamma':[0.5],'colsample_bytree':[1]}\n",
    "gsearch1 = GridSearchCV(estimator = model,param_grid = param_test1,scoring='roc_auc',n_jobs=4,iid=False,cv=5)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.grid_scores_,gsearch1.best_params_,gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#things to do\n",
    "    #try messing around with the order of the data cleaning functions\n",
    "    #try creating new columns that may have predictive value based on data we have\n",
    "    #try gridsearchcv to optimize parameters of XGBoost\n",
    "    #Run on more than 2 trials for each different dataset w/ gridsearch to see which performs the best\n",
    "    #figure out how to define the normalization data cleaning method\n",
    "    #ask Tao about other shit we could do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Testing scripts for skew handling/normlization\n",
    "\n",
    "\n",
    "file=pd.read_excel('default_of_credit_card_clients.xlsx',header=1)\n",
    "\n",
    "#Find skewness of columns\n",
    "for col in range(6,24):\n",
    "    #Need to set negative values to 0 for square roots to work\n",
    "    file.iloc[:, col][file.iloc[:, col]<0]=0\n",
    "    if abs(scipy.stats.skew(file.iloc[:,col]))<=.5:\n",
    "        print(mean.columns[col],scipy.stats.skew(mean.iloc[:,col]),' - Symmetrical')\n",
    "        pass\n",
    "    elif scipy.stats.skew(file.iloc[:,col])<(-.5) and scipy.stats.skew(file.iloc[:,col]) >=(-1):\n",
    "        print(file.columns[col],scipy.stats.skew(file.iloc[:,col]),' - Moderately Negatively Skewed')\n",
    "    elif scipy.stats.skew(file.iloc[:,col])>.5 and scipy.stats.skew(file.iloc[:,col]) <1:\n",
    "        print(file.columns[col],scipy.stats.skew(file.iloc[:,col]),' - Moderately Positively Skewed')\n",
    "    elif scipy.stats.skew(file.iloc[:,col])<-1:\n",
    "        print(file.columns[col],scipy.stats.skew(file.iloc[:,col]),' - Highly Negatively Skewed')\n",
    "    elif scipy.stats.skew(file.iloc[:,col])>1:\n",
    "        print(file.columns[col],scipy.stats.skew(file.iloc[:,col]),' - Highly Positively Skewed')\n",
    "        \n",
    "\n",
    "#Different methods for handling positive skewness - none seem to be bringing it down enough\n",
    "print(abs(scipy.stats.skew(np.sqrt(file['PAY_0']))))\n",
    "print(abs(scipy.stats.skew(np.cbrt(file['PAY_0']))))\n",
    "print(abs(scipy.stats.skew(np.log(file['PAY_0']+.0001))))\n",
    "print(abs(scipy.stats.skew(1/(file['PAY_0']+.0001))))\n",
    "\n",
    "\n",
    "#Function to automatically test different skew handling techniques and choose the one that minimizes skew\n",
    "#for col in range(6,24):\n",
    "#    minvalue=min(abs(scipy.stats.skew(np.sqrt(file[col]))),abs(scipy.stats.skew(np.cbrt(file[col]))),abs(scipy.stats.skew(np.log(file[col]))))\n",
    "#    if minvalue==abs(scipy.stats.skew(np.sqrt(file[col]))):\n",
    "#        file[col]=np.sqrt(file[col])\n",
    "#    elif minvalue==abs(scipy.stats.skew(np.cbrt(file[col]))):\n",
    "#        file[col]=np.cbrt(file[col])\n",
    "#    elif minvalue==abs(scipy.stats.skew(np.log(file[col]))):\n",
    "#        file[col]=np.log(file[col])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
