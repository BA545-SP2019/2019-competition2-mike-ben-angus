{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competition 2 ###\n",
    "\n",
    "This initial report references the following file: [Competition2](Competition2.ipynb)\n",
    "\n",
    "### Question ###\n",
    "In this competition we were trying to gain understanding and successful application of \"Adavanced Preperation of Financial Data\". In this project, we are aiming to predict if the client of a credit card company is going to make their monlthy payment. We are planning on achieving this by using both prior history of payments and their demogrpahic catagory. \n",
    "\n",
    "There are of course many factors that affect both the likelihood of a client making their payments on time. There level of education, relationship status, sex and age are the main demogrpahic catagoristics we are going to look at. \n",
    "\n",
    "In our project we are going to test our data several times with several different affecting factors. The aim was to gain the best possible performance out of our dataset.\n",
    "\n",
    "\n",
    "### Cleaning the Data  [Data Cleaning](Competition2.ipynb#Data Cleaning)<br/>\n",
    "\n",
    "We initially cleaned the data. We did this with the use of the data dictionary.\n",
    "We used the data dictionary to understand what values should be in each column. This was particularly relevant in the columns that were made up of qualitative data. Such as sex, age, and education. The cleaning of the data has been interesting for a couple of reasons... Firstly, marital status should have only contained the numbers 1 or 2. Thus, married or not married. We were getting the number 0 fairly often. We decided to change all of the 0's to the number 3 and they would fall under the catagory of \"others\". The second interesting column that needed cleaning was payment status. Some people had negative numbers. Meaning they had paid their bills off early. We decided to change all negative numbers to 0 to indicate that they had simply paid on time. The third column we took note of was the education column. According to the data dictionary it should only go up to 4- although we noticed many values above that. As a result, we converted any value above 4 to a 4 for that column. \n",
    "\n",
    "In addition to correcting data issues, we also took a variety of data manipulation steps in order to improved model performance. The four major steps we took to manipulate the data were:\n",
    "    1. Find & replace outliers on continous columns, using IQR strategy\n",
    "    2. Use Min Max Scaler on continous columns\n",
    "    3. Calculate Z scores on continous columns\n",
    "    4. One hot encode all categorical columns\n",
    "    5. Use the SMOTE algorithm to oversample the training dataset due to the imbalanced target variable\n",
    "    \n",
    "    \n",
    "### Data Pipelines [Data_Pipelines](Competition2.ipynb#Data_Pipelines)<br/>\n",
    "Now we employed either method 2 or 3- but not both together. We built multiple pipelines & in every pipeline the data cleaning methods were called to correct the data. After that was done we would experiment by calling methods 1-4. If we had more time we would continue to expand the number of data manipulation methods. The way it was developped would make it very simple to run different data manipulation steps in different orders- in order to test and improve performance. In our dataset, since the only real divergence in the path was whether we left the continous data on a 0-1 scale or used Z-scores- a lot of this was largely unneccesary. \n",
    "\n",
    "### Modeling [XGBoost](Competition2.ipynb#XGBoost)<br/>\n",
    "\n",
    "For our initial modeling effort we have decided to run two different types of models. These are the Random Forest model & the XGBoost model. Most of our focus was spent on the XGBoost model- although many of the steps taking on this model will be replicated for the Random Forest model in order to determine the best performing model. The first thing we did- was train and test a XGBoost model on the standard dataset vs. the best combination of prepped dataset we could come up with. The difference in performance over 10 trials was about 2% between the prepped & non-prepped dataset. After entering the parameters from the gridsearch & plotting the feature importance I see that Bill AMT, LIMIT BAL, and PAY 0 are our 3 most important features. We have a solid average accuracy of 82% over 10 trials- although our F1 score is much lower at around 47%. This is because 77% of the respondants in the dataset have not defaulted- so guessing non-default everytime yields good accuracy in itself. I think in order to compensate for this I need a better strategy in regards to modeling towards an unbalanced target. \n",
    "\n",
    "\n",
    "### GridSearch [GridSearch](Competition2.ipynb#GridSearch)<br/>\n",
    "We then proceeded to run a gridsearch to tune the best parameters of the XGBoost algorithm. That took quite some time to run- but once it did we hard coded those parameters into the model we use for the 10 trials. One question I still have is would the tuned parameters be different if I had a different random state in my train test split? This might be something work looking into further down the road so we can make sure we are tuning our parameters not just to the best settings for the current train/test split- but the best parameters for all train/test splits.\n",
    "### RandomForest [RandomForest](Competition2.ipynb#RandomForest)<br/>\n",
    "I also ran, one time, a Random Forest Classifier. This accuracy was slightly lower than the accuracy I received from the XGBoost model- but it's possible a gridsearch will improve that. It was interesting to note that with this model PAY 0, PAY 2, and PAY 5 were our most important features. With this model the accuracy was ~80% but the F1 score was much lower around 25%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "As we briefly mentioned above it is vital, that not only do we get our models to work, it is imperative that we then implement these models so that we can help our client to maximise their profits. When it comes to our particular case we are looking at a credit card company and their client's default rates. Within our data set, we have certain criteria that are going to allow us to put a correlation on characteristics that mean people are going to pay their credit card bill late. This will allow us to recommend to the parent company the clients that they should and shouldn’t give credit to. \n",
    "\n",
    "Our XG Boost model was our most accurate. This produced an accuracy of .80 and an F1 of .50. We would be very unlikely to give these direct results to the client. \n",
    "\n",
    "Taking a deeper dive into the data and using our XG Boost model we have established that the most important features when it comes to credit card defaults are F0 and F2 (X1 & X6) their feature importance was at least double the third highest feature. X1 = ‘Limit Balance’ X6 = ‘Pay 0’. The limit balance was always going to be one of the main features this is due to the fact that it was the amount of credit given to a client. The X6 feature signifies whether a client made his payment in September 2005, the first month on record. Essentially what this is saying is if clients couldn’t make their first payment on record they were generally struggling to catch up. The third main feature is X11 which is pay amount 6. This is the last month on record of whether people paid their credit card bill. \n",
    "\n",
    "All of the above information would help the credit card company for several reasons. Firstly, is this new customer likely to fall into the bracket of being unable to pay their credit card bill? It shows that starting people with smaller amounts of credit is a safe option. Of course, credit card companies already employ this policy and increase peoples credit as they become trusted customers. There is, of course, another plan of action which would allow clients to see our findings. This would allow them to make informed decisions about what credit card limits they should look to find and how much they would wish to spend on their credit card per month. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
